{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# честно, подсмотрел тут https://habr.com/ru/post/321834/, \n",
    "# всё делать не стал - мощностей не хватает и времени\n",
    "# понравился подход, описанный в статье для генерации бОльших данных из небольшого набора (я уже ситал о таком, но тут увидел живьём)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense\n",
    "from tensorflow.keras.layers import  BatchNormalization\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['котэ', 'пёсель']\n",
    "image_size = (150, 150)\n",
    "chanel_n = 3\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transformer = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 2 classes.\n",
      "Found 8998 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# в data есть 2 папки train и valid с подпапками изображений (cat и dog)\n",
    "train_gen = img_transformer.flow_from_directory(\n",
    "    'data/train', \n",
    "    target_size=image_size, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "valid_gen = img_transformer.flow_from_directory(\n",
    "    'data/valid', \n",
    "    target_size=image_size, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = applications.VGG16(weights ='imagenet', include_top=False, input_shape=(image_size[0], image_size[1], chanel_n ))\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 4, 4, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                524352    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 15,245,313\n",
      "Trainable params: 529,601\n",
      "Non-trainable params: 14,715,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.losses.BinaryCrossentropy(), optimizer='rmsprop', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 160 steps, validate for 90 steps\n",
      "Epoch 1/15\n",
      "159/160 [============================>.] - ETA: 20s - loss: 0.5217 - accuracy: 0.7736"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 5374s 34s/step - loss: 0.5208 - accuracy: 0.7740 - val_loss: 0.8003 - val_accuracy: 0.6327\n",
      "Epoch 2/15\n",
      "160/160 [==============================] - 4519s 28s/step - loss: 0.1580 - accuracy: 0.9472 - val_loss: 1.1306 - val_accuracy: 0.6553\n",
      "Epoch 3/15\n",
      "160/160 [==============================] - 3235s 20s/step - loss: 0.0976 - accuracy: 0.9722 - val_loss: 0.9993 - val_accuracy: 0.7087\n",
      "Epoch 4/15\n",
      "160/160 [==============================] - 2992s 19s/step - loss: 0.0705 - accuracy: 0.9799 - val_loss: 1.4294 - val_accuracy: 0.6885\n",
      "Epoch 5/15\n",
      "160/160 [==============================] - 3402s 21s/step - loss: 0.0670 - accuracy: 0.9837 - val_loss: 1.6877 - val_accuracy: 0.6647\n",
      "Epoch 6/15\n",
      "160/160 [==============================] - 3095s 19s/step - loss: 0.0607 - accuracy: 0.9847 - val_loss: 1.5323 - val_accuracy: 0.7288\n",
      "Epoch 7/15\n",
      "160/160 [==============================] - 3874s 24s/step - loss: 0.0529 - accuracy: 0.9860 - val_loss: 1.9627 - val_accuracy: 0.6942\n",
      "Epoch 8/15\n",
      "160/160 [==============================] - 3238s 20s/step - loss: 0.0441 - accuracy: 0.9876 - val_loss: 2.1194 - val_accuracy: 0.6818\n",
      "Epoch 9/15\n",
      "160/160 [==============================] - 2841s 18s/step - loss: 0.0361 - accuracy: 0.9900 - val_loss: 2.2403 - val_accuracy: 0.7054\n",
      "Epoch 10/15\n",
      "160/160 [==============================] - 2822s 18s/step - loss: 0.0365 - accuracy: 0.9923 - val_loss: 2.4520 - val_accuracy: 0.6899\n",
      "Epoch 11/15\n",
      "160/160 [==============================] - 2852s 18s/step - loss: 0.0377 - accuracy: 0.9902 - val_loss: 2.6349 - val_accuracy: 0.6875\n",
      "Epoch 12/15\n",
      "160/160 [==============================] - 2877s 18s/step - loss: 0.0345 - accuracy: 0.9923 - val_loss: 2.7229 - val_accuracy: 0.6699\n",
      "Epoch 13/15\n",
      "160/160 [==============================] - 2848s 18s/step - loss: 0.0424 - accuracy: 0.9902 - val_loss: 3.0774 - val_accuracy: 0.6398\n",
      "Epoch 14/15\n",
      "160/160 [==============================] - 2972s 19s/step - loss: 0.0329 - accuracy: 0.9917 - val_loss: 3.2146 - val_accuracy: 0.6710\n",
      "Epoch 15/15\n",
      "160/160 [==============================] - 2821s 18s/step - loss: 0.0272 - accuracy: 0.9931 - val_loss: 3.8206 - val_accuracy: 0.6481\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "hist = model.fit(\n",
    "    train_gen,\n",
    "    epochs=15,\n",
    "    validation_data=valid_gen,\n",
    "    workers=4,\n",
    "    verbose=1\n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "74/90 [=======================>......] - ETA: 3:07 - loss: 4.2218 - accuracy: 0.6008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 1053s 12s/step - loss: 3.8206 - accuracy: 0.6481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.820637869503763, 0.648144]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2e879d1a8934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hist' is not defined"
     ]
    }
   ],
   "source": [
    "# пришлось перезапустить блокнот, т.к. кончилась память - никакой истории тут, конечно, нет, но там всё было грустно\n",
    "'''\n",
    "plt.plot(hist.history['val_accuracy'],'r',label='val_accuracy')\n",
    "plt.plot(hist.history['accuracy'],'g',label='train_accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как-то всё печально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# попробуем немного иначе\n",
    "# загрузим готовые веса и попробуем обучить модель, используя SGD (он, вроде бы, менее агрессивный... чуть более тонкая настройка)\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(64, activation=tf.nn.relu))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, activation=tf.nn.sigmoid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.losses.BinaryCrossentropy(), optimizer='SGD', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 160 steps, validate for 90 steps\n",
      "Epoch 1/3\n",
      "159/160 [============================>.] - ETA: 12s - loss: 0.0069 - accuracy: 0.9982"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n",
      "  \" Skipping tag %s\" % (size, len(data), tag)\n",
      "/anaconda3/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 2978s 19s/step - loss: 0.0069 - accuracy: 0.9983 - val_loss: 3.3843 - val_accuracy: 0.6710\n",
      "Epoch 2/3\n",
      "160/160 [==============================] - 3721s 23s/step - loss: 0.0049 - accuracy: 0.9989 - val_loss: 3.3296 - val_accuracy: 0.6755\n",
      "Epoch 3/3\n",
      "160/160 [==============================] - 3433s 21s/step - loss: 0.0048 - accuracy: 0.9989 - val_loss: 3.1886 - val_accuracy: 0.6862\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(\n",
    "    train_gen,\n",
    "    epochs=3,\n",
    "    validation_data=valid_gen,\n",
    "    workers=4,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "90/90 [==============================] - 1075s 12s/step - loss: 3.1886 - accuracy: 0.6862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.1886267807748583, 0.68615246]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(valid_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history['val_accuracy'],'r',label='val_accuracy')\n",
    "plt.plot(hist.history['accuracy'],'g',label='train_accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Графики выглядят не очень, точность модели низковата. Хотя ситуация изменилась в лучшую сторону (прошлый результат был ниже) всего за 3 эпохи.  \n",
    "Думаю, нужно получше поделить данные и добавить аугментацию, чтобы расширить обучающую выборку. К сожалению, на моём железе 15 эпох обучения модели занимают около 8-10 часов, что довольно долго.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
